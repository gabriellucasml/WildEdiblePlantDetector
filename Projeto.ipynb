{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b48cdc4",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59202bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and hyperparameters\n",
    "MAX_WORD_INDEX = 10000\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "NUM_LSTM_UNITS = 32\n",
    "DROPOUT_RATE = 0.2\n",
    "LR = 0.001\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1.0e-8\n",
    "DECAY = 0.0\n",
    "VAL_PERC = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373225ae",
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "if(os.path.isdir(os.getcwd()+'/dataset/processed') == False):\n",
    "    species = pd.read_csv('class_names.csv').values.tolist()\n",
    "    species.insert(0,['Alfafa'])\n",
    "    i = 0\n",
    "    os.makedirs(os.getcwd()+'/dataset/processed', exist_ok=True)\n",
    "    for dirname, _, filenames in os.walk(os.getcwd()+'/dataset/resized'):\n",
    "        cur_specie = species[i][0]\n",
    "        class_dir = os.getcwd()+'/dataset/processed/'+cur_specie\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        img_count = 1\n",
    "        for filename in filenames:\n",
    "            image = Image.open(os.path.join(dirname, filename))\n",
    "            img_width, img_height = image.size\n",
    "            if(img_width == img_height):\n",
    "                image = image.resize((256,256))\n",
    "            elif(img_width > img_height):\n",
    "                image = image.resize((img_width, 256)).crop((0,0,256,256))\n",
    "            else:\n",
    "                image = image.resize((256, img_height)).crop((0,0,256,256))\n",
    "            image.save(class_dir+'/'+cur_specie+str(img_count)+'.png')\n",
    "            img_count += 1\n",
    "        if(dirname != os.getcwd()+'/dataset/resized'):\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45400fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(path):\n",
    "    data = []\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            image = Image.open(os.path.join(dirname, filename))\n",
    "            data.append([pd.array(image.getdata()),dirname])\n",
    "    return pd.DataFrame(data)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25c6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16526 files belonging to 35 classes.\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 35), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "dataset = image_dataset_from_directory(os.getcwd()+'/dataset/processed', \n",
    "                                       labels='inferred',\n",
    "                                       label_mode='categorical',\n",
    "                                       batch_size=256)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()\n",
    "images = list(dataset.map(lambda x, y:x))\n",
    "labels = list(dataset.map(lambda x, x:y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8a5293",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# seed random number generator\u001b[39;00m\n\u001b[0;32m      6\u001b[0m seed(datetime\u001b[38;5;241m.\u001b[39mnow())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "# seed random number generator\n",
    "seed(datetime.now())\n",
    "rand = randint(0,16526)\n",
    "Image.fromarray(dataset[0][rand]).show()\n",
    "print(dataset[1][rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2c3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_cnn'\n",
    "\n",
    "history_file = os.path.join(model_name, f'history_{model_name}.csv')\n",
    "logdir = os.path.join(model_name, 'log')\n",
    "ckpts = os.path.join(model_name, 'ckpts')\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "os.makedirs(ckpts, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff1f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset[0],dataset[1],test_size = 0.4)\n",
    "\n",
    "del dataset\n",
    "\n",
    "#reashaping and normalizing\n",
    "#X_train = np.array(X_train/255.0)\n",
    "#X_test = np.array(X_test/255.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47876da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 254, 254, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 127, 127, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 125, 125, 32)      18464     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 62, 62, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 123008)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 70)                8610630   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 35)                2485      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,633,371\n",
      "Trainable params: 8,633,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),input_shape = (256,256,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(70))\n",
    "model.add(Dense(35, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f9d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "# set optimizer\n",
    "opt = optimizers.Adam(learning_rate=LR,\n",
    "    beta_1=BETA1,\n",
    "    beta_2=BETA2,\n",
    "    epsilon=EPSILON,\n",
    "    decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad000beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss and metrics\n",
    "loss = losses.binary_crossentropy\n",
    "met = [metrics.binary_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24d31f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model: optimization method, training criterion and metrics\n",
    "model.compile(\n",
    "  optimizer=opt,\n",
    "  loss=loss,\n",
    "  metrics=met\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "012c3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stop, save best checkpoint\n",
    "filepath = ckpts + '/weights-improvement-{epoch:02d}-{val_binary_accuracy:.4f}.hdf5'\n",
    "callbacks_list = [\n",
    "  EarlyStopping(\n",
    "    monitor='binary_accuracy',\n",
    "    patience=10),\n",
    "  ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='val_binary_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1),\n",
    "  TensorBoard(\n",
    "    log_dir=logdir),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbd1fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation\n",
    "nsamples = X_train.shape[0]\n",
    "nval_samples = int(VAL_PERC * nsamples)\n",
    "X_val = X_train[:nval_samples]\n",
    "partial_X_train = X_train[nval_samples:]\n",
    "y_val = y_train[:nval_samples]\n",
    "partial_y_train = y_train[nval_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4993d45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##print(\"X\", partial_X_train)\n",
    "##print(\"y\", partial_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "652d68e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_X_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpartial_y_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programs\\Anaconda\\envs\\IA\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Programs\\Anaconda\\envs\\IA\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(partial_X_train,\n",
    "  partial_y_train,\n",
    "  epochs=NUM_EPOCHS,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  validation_data=(X_val, y_val),\n",
    "  callbacks=callbacks_list,\n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36526ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.038px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 287.85,
   "position": {
    "height": "309.85px",
    "left": "518.2px",
    "right": "20px",
    "top": "111px",
    "width": "398px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
