{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T7tUJeNU3NPx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 18:26:01.601954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:01.601984: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import models, layers, optimizers, losses, metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AD1okPDL6mUi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# constants and hyperparameters\n",
    "MAX_WORD_INDEX = 10000\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "NUM_LSTM_UNITS = 32\n",
    "DROPOUT_RATE = 0.2\n",
    "LR = 0.001\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1.0e-8\n",
    "DECAY = 0.0\n",
    "VAL_PERC = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nkpFdf2A7Qe_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# process input arguments\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--model_name', \n",
    "#                    type=str,\n",
    "#                    default='model_lstm', \n",
    "#                    help='--model_name=<model1|model2|...>')\n",
    "#args = vars(parser.parse_args())\n",
    "model_name\t= 'model_lstm'\n",
    "\n",
    "#print(f'\\nModel type: {model_name}')\n",
    "\n",
    "history_file = os.path.join(model_name, f'history_{model_name}.csv')\n",
    "logdir = os.path.join(model_name, 'log')\n",
    "ckpts = os.path.join(model_name, 'ckpts')\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "os.makedirs(ckpts, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5355,
     "status": "ok",
     "timestamp": 1656735085250,
     "user": {
      "displayName": "Gabriel Lucas Medeiros Leite",
      "userId": "08534388749759684636"
     },
     "user_tz": 180
    },
    "id": "Q1LaE-YY8Rm_",
    "outputId": "e7b984f0-94ac-47c4-e009-0c42285b7290",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load database using Keras\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = MAX_WORD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1656735086689,
     "user": {
      "displayName": "Gabriel Lucas Medeiros Leite",
      "userId": "08534388749759684636"
     },
     "user_tz": 180
    },
    "id": "MIiFaVlw-iIn",
    "outputId": "9f5ba768-cc4a-48f2-9ae4-b8abbfdecc3c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum train sequence length: 2494\n",
      "Maximum test sequence length: 2315\n",
      "Minimum train sequence length: 11\n",
      "Minimum test sequence length: 7\n"
     ]
    }
   ],
   "source": [
    "#  print some information on the data\n",
    "max_seq_len_train = max([len(sequence) for sequence in train_data])\n",
    "max_seq_len_test = max([len(sequence) for sequence in test_data])\n",
    "min_seq_len_train = min([len(sequence) for sequence in train_data])\n",
    "min_seq_len_test = min([len(sequence) for sequence in test_data])\n",
    "print(f'Maximum train sequence length: {max_seq_len_train}')\n",
    "print(f'Maximum test sequence length: {max_seq_len_test}')\n",
    "print(f'Minimum train sequence length: {min_seq_len_train}')\n",
    "print(f'Minimum test sequence length: {min_seq_len_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1377,
     "status": "ok",
     "timestamp": 1656735089331,
     "user": {
      "displayName": "Gabriel Lucas Medeiros Leite",
      "userId": "08534388749759684636"
     },
     "user_tz": 180
    },
    "id": "BMws5gaR-pQb",
    "outputId": "d8fec122-106a-40f3-e38c-64e8ce429028",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (25000, 2494)\n",
      "X_test shape: (25000, 2315)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(train_data)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(test_data)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1656735090363,
     "user": {
      "displayName": "Gabriel Lucas Medeiros Leite",
      "userId": "08534388749759684636"
     },
     "user_tz": 180
    },
    "id": "I-LHwQGR-2fD",
    "outputId": "4f9d300d-7cfc-4b63-fb54-78a2e491c8e2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (25000,)\n",
      "y_test shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# transform labels  into arrays\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test  = np.asarray(test_labels).astype('float32')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1656735093328,
     "user": {
      "displayName": "Gabriel Lucas Medeiros Leite",
      "userId": "08534388749759684636"
     },
     "user_tz": 180
    },
    "id": "1MVZtCxZ_O3Y",
    "outputId": "86abbbf2-38a4-4825-acbd-a146ddf4aec4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 18:26:21.086718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-05 18:26:21.087022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087096: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087202: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087360: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-05 18:26:21.087417: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-05 18:26:21.121724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                20608     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,641\n",
      "Trainable params: 1,300,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(MAX_WORD_INDEX, EMBEDDING_DIM))\n",
    "\"\"\"\"\n",
    "model.add(layers.LSTM(\n",
    "    units=NUM_LSTM_UNITS,\n",
    "    dropout=DROPOUT_RATE,\n",
    "    recurrent_dropout=DROPOUT_RATE,\n",
    "    return_sequences=True\n",
    "  ))\n",
    "\"\"\"\n",
    "model.add(layers.LSTM(\n",
    "                      units=NUM_LSTM_UNITS,\n",
    "                      dropout=DROPOUT_RATE,\n",
    "                      recurrent_dropout=DROPOUT_RATE\n",
    "                      ))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m9YDos_o_zYF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "opt = optimizers.Adam(learning_rate=LR,\n",
    "    beta_1=BETA1,\n",
    "    beta_2=BETA2,\n",
    "    epsilon=EPSILON,\n",
    "    decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H6-vdi5cAEDH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set loss and metrics\n",
    "loss = losses.binary_crossentropy\n",
    "met = [metrics.binary_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NKzcYI5SAO1f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compile model: optimization method, training criterion and metrics\n",
    "model.compile(\n",
    "  optimizer=opt,\n",
    "  loss=loss,\n",
    "  metrics=met\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kMZxbjsfATrX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# early stop, save best checkpoint\n",
    "filepath = ckpts + '/weights-improvement-{epoch:02d}-{val_binary_accuracy:.4f}.hdf5'\n",
    "callbacks_list = [\n",
    "  EarlyStopping(\n",
    "    monitor='binary_accuracy',\n",
    "    patience=10),\n",
    "  ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='val_binary_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1),\n",
    "  TensorBoard(\n",
    "    log_dir=logdir),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2fFmDOTfAn9t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split training data into training and validation\n",
    "nsamples = X_train.shape[0]\n",
    "nval_samples = int(VAL_PERC * nsamples)\n",
    "X_val = X_train[:nval_samples]\n",
    "partial_X_train = X_train[nval_samples:]\n",
    "y_val = y_train[:nval_samples]\n",
    "partial_y_train = y_train[nval_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "BogvGS7vD8QK",
    "outputId": "b84bb3bc-656a-47ce-c774-7aa111f7090c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 15:29:55.473921: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n",
      "2022-07-05 15:29:55.495958: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n",
      "2022-07-05 15:29:59.039662: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n",
      "2022-07-05 15:29:59.649563: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/118 [..............................] - ETA: 18:21 - loss: 0.6931 - binary_accuracy: 0.4766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 15:30:03.377202: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - ETA: 0s - loss: 0.5090 - binary_accuracy: 0.7489\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.83520, saving model to model_lstm/ckpts/weights-improvement-01-0.8352.hdf5\n",
      "118/118 [==============================] - 325s 3s/step - loss: 0.5090 - binary_accuracy: 0.7489 - val_loss: 0.3902 - val_binary_accuracy: 0.8352\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2729 - binary_accuracy: 0.8955\n",
      "Epoch 2: val_binary_accuracy improved from 0.83520 to 0.87730, saving model to model_lstm/ckpts/weights-improvement-02-0.8773.hdf5\n",
      "118/118 [==============================] - 315s 3s/step - loss: 0.2729 - binary_accuracy: 0.8955 - val_loss: 0.2994 - val_binary_accuracy: 0.8773\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1801 - binary_accuracy: 0.9365\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 308s 3s/step - loss: 0.1801 - binary_accuracy: 0.9365 - val_loss: 0.3224 - val_binary_accuracy: 0.8677\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1720 - binary_accuracy: 0.9384\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 303s 3s/step - loss: 0.1720 - binary_accuracy: 0.9384 - val_loss: 0.4017 - val_binary_accuracy: 0.8527\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1261 - binary_accuracy: 0.9566\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 302s 3s/step - loss: 0.1261 - binary_accuracy: 0.9566 - val_loss: 0.3598 - val_binary_accuracy: 0.8692\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0845 - binary_accuracy: 0.9718\n",
      "Epoch 6: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 301s 3s/step - loss: 0.0845 - binary_accuracy: 0.9718 - val_loss: 0.4169 - val_binary_accuracy: 0.8633\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0697 - binary_accuracy: 0.9784\n",
      "Epoch 7: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 301s 3s/step - loss: 0.0697 - binary_accuracy: 0.9784 - val_loss: 0.4537 - val_binary_accuracy: 0.8592\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0586 - binary_accuracy: 0.9823\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 303s 3s/step - loss: 0.0586 - binary_accuracy: 0.9823 - val_loss: 0.5104 - val_binary_accuracy: 0.8439\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0724 - binary_accuracy: 0.9765\n",
      "Epoch 9: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 301s 3s/step - loss: 0.0724 - binary_accuracy: 0.9765 - val_loss: 0.5929 - val_binary_accuracy: 0.8611\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0378 - binary_accuracy: 0.9895\n",
      "Epoch 10: val_binary_accuracy did not improve from 0.87730\n",
      "118/118 [==============================] - 302s 3s/step - loss: 0.0378 - binary_accuracy: 0.9895 - val_loss: 0.5785 - val_binary_accuracy: 0.8621\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(partial_X_train,\n",
    "  partial_y_train,\n",
    "  epochs=NUM_EPOCHS,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  validation_data=(X_val, y_val),\n",
    "  callbacks=callbacks_list,\n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jUypIs12ECDW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "with open(history_file, mode='w') as f:\n",
    "  history_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 57s 270ms/step - loss: 0.6213 - binary_accuracy: 0.8510\n",
      "Test score (loss): 0.6212987303733826\n",
      "Test accuracy: 0.8509600162506104\n"
     ]
    }
   ],
   "source": [
    "# score model using test data\n",
    "score, acc = model.evaluate(\n",
    "    X_test, y_test,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print('Test score (loss):', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model type: model_cnn\n"
     ]
    }
   ],
   "source": [
    "model_name\t= 'model_cnn'\n",
    "\n",
    "print(f'\\nModel type: {model_name}')\n",
    "\n",
    "history_file = os.path.join(model_name, f'history_{model_name}.csv')\n",
    "logdir = os.path.join(model_name, 'log')\n",
    "ckpts = os.path.join(model_name, 'ckpts')\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "os.makedirs(ckpts, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, None, 64)          24640     \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, None, 64)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, None, 32)          6176      \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, None, 32)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,311,905\n",
      "Trainable params: 1,311,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(MAX_WORD_INDEX, EMBEDDING_DIM))\n",
    "\n",
    "model.add(layers.Conv1D(64,3, activation='softmax'))\n",
    "model.add(layers.MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Conv1D(32,3, activation='softmax'))\n",
    "model.add(layers.MaxPool1D(pool_size=2))\n",
    "\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,\n",
    "    metrics=met\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 18:36:31.037872: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 149640000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 18:36:33.274666: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 163446784 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - ETA: 0s - loss: 0.6934 - binary_accuracy: 0.4989\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.50530, saving model to model_lstm/ckpts/weights-improvement-01-0.5053.hdf5\n",
      "118/118 [==============================] - 119s 986ms/step - loss: 0.6934 - binary_accuracy: 0.4989 - val_loss: 0.6930 - val_binary_accuracy: 0.5053\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.6605 - binary_accuracy: 0.6108\n",
      "Epoch 2: val_binary_accuracy improved from 0.50530 to 0.76070, saving model to model_lstm/ckpts/weights-improvement-02-0.7607.hdf5\n",
      "118/118 [==============================] - 114s 964ms/step - loss: 0.6605 - binary_accuracy: 0.6108 - val_loss: 0.5397 - val_binary_accuracy: 0.7607\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.4246 - binary_accuracy: 0.8111\n",
      "Epoch 3: val_binary_accuracy improved from 0.76070 to 0.83240, saving model to model_lstm/ckpts/weights-improvement-03-0.8324.hdf5\n",
      "118/118 [==============================] - 114s 964ms/step - loss: 0.4246 - binary_accuracy: 0.8111 - val_loss: 0.3799 - val_binary_accuracy: 0.8324\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2837 - binary_accuracy: 0.8852\n",
      "Epoch 4: val_binary_accuracy improved from 0.83240 to 0.84830, saving model to model_lstm/ckpts/weights-improvement-04-0.8483.hdf5\n",
      "118/118 [==============================] - 113s 963ms/step - loss: 0.2837 - binary_accuracy: 0.8852 - val_loss: 0.3480 - val_binary_accuracy: 0.8483\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1899 - binary_accuracy: 0.9294\n",
      "Epoch 5: val_binary_accuracy improved from 0.84830 to 0.85720, saving model to model_lstm/ckpts/weights-improvement-05-0.8572.hdf5\n",
      "118/118 [==============================] - 113s 962ms/step - loss: 0.1899 - binary_accuracy: 0.9294 - val_loss: 0.3485 - val_binary_accuracy: 0.8572\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.1242 - binary_accuracy: 0.9589\n",
      "Epoch 6: val_binary_accuracy did not improve from 0.85720\n",
      "118/118 [==============================] - 113s 962ms/step - loss: 0.1242 - binary_accuracy: 0.9589 - val_loss: 0.3813 - val_binary_accuracy: 0.8511\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0791 - binary_accuracy: 0.9769\n",
      "Epoch 7: val_binary_accuracy did not improve from 0.85720\n",
      "118/118 [==============================] - 114s 963ms/step - loss: 0.0791 - binary_accuracy: 0.9769 - val_loss: 0.4150 - val_binary_accuracy: 0.8541\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0465 - binary_accuracy: 0.9884\n",
      "Epoch 8: val_binary_accuracy did not improve from 0.85720\n",
      "118/118 [==============================] - 116s 981ms/step - loss: 0.0465 - binary_accuracy: 0.9884 - val_loss: 0.4573 - val_binary_accuracy: 0.8498\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0268 - binary_accuracy: 0.9947\n",
      "Epoch 9: val_binary_accuracy did not improve from 0.85720\n",
      "118/118 [==============================] - 112s 953ms/step - loss: 0.0268 - binary_accuracy: 0.9947 - val_loss: 0.4986 - val_binary_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.0154 - binary_accuracy: 0.9981\n",
      "Epoch 10: val_binary_accuracy did not improve from 0.85720\n",
      "118/118 [==============================] - 112s 947ms/step - loss: 0.0154 - binary_accuracy: 0.9981 - val_loss: 0.5419 - val_binary_accuracy: 0.8479\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "with open(history_file, mode='w') as f:\n",
    "    history_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 34s 173ms/step - loss: 0.5860 - binary_accuracy: 0.8384\n",
      "Test score (loss): 0.5860422849655151\n",
      "Test accuracy: 0.8383600115776062\n"
     ]
    }
   ],
   "source": [
    "# score model using test data\n",
    "score, acc = model.evaluate(\n",
    "    X_test, y_test,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print('Test score (loss):', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNr9VzygoCzf1UbVn3gDqq/",
   "name": "NN.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
